{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 5: Embeddings! \n",
    "\n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing new packages\n",
    "\n",
    "--------------------------------------------------\n",
    "**READ/SKIM THIS WHOLE CELL BEFORE RUNNING COMMANDS** :)\n",
    "\n",
    "We need to install pytorch in our current environment. We also need to download the [BERT](https://huggingface.co/google-bert/bert-large-uncased) model, and we will do so from [HuggingFace](https://huggingface.co/models/). \n",
    "\n",
    "##### **Recommended**: install a new environment for this assignment. \n",
    "\n",
    "- If you're on an [Apple Silicon](https://support.apple.com/en-us/116943) (M1/M2/etc.) Mac, we recommend using the ARM64-optimized packages for best performance.\n",
    "    ```\n",
    "    CONDA_SUBDIR=osx-arm64 conda env create -f environment_pa5.yml\n",
    "    conda activate cs124_pa5\n",
    "    conda config --env --set subdir osx-arm64\n",
    "    ```\n",
    " \n",
    "    \n",
    "- If you are on an Intel Mac, use the following commands instead\n",
    "    ```\n",
    "    CONDA_SUBDIR=osx-64 conda env create -f environment_pa5.yml\n",
    "    conda activate cs124_pa5\n",
    "    ```\n",
    "    \n",
    "- Otherwise, run the following (Windows/Linux): \n",
    "    ```\n",
    "    conda env create -f environment_pa5.yml\n",
    "    conda activate cs124_pa5\n",
    "    ```\n",
    "\n",
    "To use these new packages, change your kernel to use this new package version set. (Kernel -> Change Kernel)\n",
    "\n",
    "Both of these environments now contain pytorch and huggingface in addition to the existing packages we had. To verify they installed, run the following cell:\n",
    "\n",
    "\n",
    "##### **Less Recommended**: install into existing environment\n",
    "\n",
    "To install these packages into our existing python environment, you can run the following commands:\n",
    "\n",
    "```\n",
    "conda activate cs124\n",
    "conda install -c pytorch pytorch\n",
    "conda install -c huggingface transformers\n",
    "```\n",
    "To use these new packages, you may need to restart your kernel (Kernel > Restart)\n",
    "\n",
    "- This method is not as recommended because of the general principle \"If it ain't broke, don't fix it\". And specifically on M1+ macs, you will likely run into an issue of pytorch not working because your existing cs124 environment is running through the Rosetta 2 translation layer, meaning pytorch can't detect your hardware. But if you know how to troubleshoot issues you may come across, this method should work just fine.\n",
    "\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This BERT model uses about 440MB of storage, but is deletable after the assignment\n",
    "from transformers import BertTokenizer, BertModel, file_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "except:\n",
    "    print(\"Error occurred. Did pytorch install correctly? Reach out to us on Ed for help.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell, please just run it!\n",
    "import quizlet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Mission\n",
    "The assignment consists of five parts. The first half deals with static embeddings, while the later half will use contextual embeddings. You don't have to worry if you haven't learned about transformers or BERT just yet, this assignment will walk you through the basics on how to use these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Static Embeddings\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "You’ll be using subset of ~4k 50-dimensional GloVe embeddings trained on Wikipedia articles. The GloVe (Global Vectors) model learns vector representations for words by looking at global word-word co-occurrence statistics in a body of text and learning vectors such that their dot product is proportional to the probability of the corresponding words co-occuring in a piece of text. The GloVe model was developed right here at Stanford, and if you’re curious you can read more about it [here](https://nlp.stanford.edu/projects/glove/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Synonyms\n",
    "For this section, your goal is to answer questions of the form:\n",
    "\n",
    "- What is a synonym for `warrior`?  \n",
    "  - soldier\n",
    "  - sailor\n",
    "  - pirate\n",
    "  - spy  \n",
    "\n",
    "You are given as input a word and a list of candidate choices. Your goal is to return the choice you think is the synonym. You’ll first implement two similarity metrics - euclidean distance and cosine similarity - then leverage them to answer the multiple choice questions!\n",
    "\n",
    "Specifically, you will implement the following 4 functions:\n",
    "\n",
    "* **euclidean_distance()**: calculate the euclidean distance between two vectors. Note: you’ll only use this metric in Part 1. For the rest of the assignment, you'll only use cosine similarity.\n",
    "* **cosine_similarity()**: calculate the cosine similarity between two vectors. You’ll be using this helper function throughout the other parts of the assignment as well, so you’ll want to get it right!\n",
    "* **find_synonym()**: given a word, a list of 4 candidate choices, and which similarity metric to use, return which word you think is the synonym! The function takes in `comparison_metric` as a parameter: if its value is `euc_dist`, you'll use Euclidean distance as the similarity metric; if its value is `cosine_sim`, you'll use cosine similarity as the metric.\n",
    "* **part1_written()**: you’ll find that finding synonyms with word embeddings works quite well, especially when using cosine similarity as the metric. However, it’s not perfect. In this function, you’ll look at a question that your `find_synonyms()` function (using cosine similarity) gets wrong, and answer why you think this might be the case. Please return your answer as a string in this function.\n",
    "\n",
    "Note: for the rest of the assignment, you'll only use cosine similarity as the comparison metric. You won't use the euclidean distance function anymore.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    '''\n",
    "    Calculates and returns the cosine similarity between vectors v1 and v2\n",
    "    Arguments:\n",
    "        v1 (np.array), v2 (np.array): vectors\n",
    "    Returns:\n",
    "        cosine_sim (float): the cosine similarity between v1, v2\n",
    "    '''\n",
    "    cosine_sim = 0\n",
    "    #########################################################\n",
    "    ## TODO: calculate cosine similarity between v1, v2    ##\n",
    "    #########################################################\n",
    "    cosine_sim = np.dot(v1, v2) / \\\n",
    "        np.prod([np.sqrt(np.sum(np.square(item))) for item in [v1, v2]])  \n",
    "    #########################################################\n",
    "    ## End TODO                                            ##\n",
    "    #########################################################\n",
    "    return cosine_sim   \n",
    "\n",
    "cosine_similarity(np.array([442, 8 ,2]), np.array([5, 3982, 3325]))\n",
    "\n",
    "def euclidean_distance(v1, v2):\n",
    "    '''\n",
    "    Calculates and returns the euclidean distance between v1 and v2\n",
    "\n",
    "    Arguments:\n",
    "        v1 (np.array), v2 (np.array): vectors\n",
    "\n",
    "    Returns:\n",
    "        euclidean_dist (float): the euclidean distance between v1, v2\n",
    "    '''\n",
    "    euclidean_dist = 0\n",
    "    #########################################################\n",
    "    ## TODO: calculate euclidean distance between v1, v2   ##\n",
    "    #########################################################\n",
    "    euclidean_dist = np.sqrt(\n",
    "        np.sum([(item[0] - item[1])*(item[0] - item[1]) \\\n",
    "                for item in np.column_stack((v1, v2))])        \n",
    "    )\n",
    "    #########################################################\n",
    "    ## End TODO                                           ##\n",
    "    #########################################################\n",
    "    return euclidean_dist                 \n",
    "\n",
    "def find_synonym(word, choices, embeddings, comparison_metric):\n",
    "    '''\n",
    "    Answer a multiple choice synonym question! Namely, given a word w \n",
    "    and list of candidate answers, find the word that is most similar to w.\n",
    "    Similarity will be determined by either euclidean distance or cosine\n",
    "    similarity, depending on what is passed in as the comparison_metric.\n",
    "\n",
    "    Arguments:\n",
    "        word (str): word\n",
    "        choices (List[str]): list of candidate answers\n",
    "        embeddings (Dict[str, np.array]): map of words to their embeddings\n",
    "        comparison_metric (str): either 'euc_dist' or 'cosine_sim'. \n",
    "            This indicates which metric to use - either euclidean distance or cosine similarity.\n",
    "            With euclidean distance, we want the word with the lowest euclidean distance.\n",
    "            With cosine similarity, we want the word with the highest cosine similarity.\n",
    "\n",
    "    Returns:\n",
    "        answer (str): the word in choices most similar to the given word\n",
    "    '''\n",
    "    answer = None\n",
    "    #########################################################\n",
    "    ## TODO: find synonym                                  ##\n",
    "    #########################################################\n",
    "    score = None\n",
    "    word_embedding = embeddings[word]\n",
    "    for item in choices:\n",
    "        candidate_embedding = embeddings[item]\n",
    "        if  comparison_metric == 'cosine_sim':\n",
    "            similarity = cosine_similarity(word_embedding, candidate_embedding)\n",
    "            if score is None or score < similarity:\n",
    "                score = similarity\n",
    "                answer = item\n",
    "                pass\n",
    "            pass\n",
    "        else:\n",
    "            similarity = euclidean_distance(word_embedding, candidate_embedding)\n",
    "            if score is None or score > similarity:\n",
    "                score = similarity\n",
    "                answer = item\n",
    "                pass                \n",
    "            pass\n",
    "        pass\n",
    "    #########################################################\n",
    "    ## End TODO                                            ##\n",
    "    ######################################################### \n",
    "    return answer\n",
    "\n",
    "def part1_written():\n",
    "    '''\n",
    "    Finding synonyms using cosine similarity on word embeddings does fairly well!\n",
    "    However, it's not perfect. In particular, you should see that it gets the last\n",
    "    synonym quiz question wrong (the true answer would be positive):\n",
    "\n",
    "    30. What is a synonym for sanguine?\n",
    "        a) pessimistic\n",
    "        b) unsure\n",
    "        c) sad\n",
    "        d) positive\n",
    "\n",
    "    What word does it choose instead? In 1-2 sentences, explain why you think \n",
    "    it got the question wrong.\n",
    "    \n",
    "    See the cell below for the code to run for this part\n",
    "    '''\n",
    "    #########################################################\n",
    "    ## TODO: replace string with your answer               ##\n",
    "    ######################################################### \n",
    "    answer = 'pessimistic' \n",
    "    #########################################################\n",
    "    ## End TODO                                            ##\n",
    "    ######################################################### \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1: Synonyms!\n",
      "-----------------\n",
      "Answering part 1 using euclidean distance as the comparison metric...\n",
      "1. What is a synonym for gullible?\n",
      "    a) unrealistic\n",
      "    b) naive\n",
      "    c) complicated\n",
      "    d) wary\n",
      "you answered: naive \n",
      "\n",
      "2. What is a synonym for counter?\n",
      "    a) parry\n",
      "    b) agree\n",
      "    c) hold\n",
      "    d) run\n",
      "you answered: hold \n",
      "\n",
      "3. What is a synonym for feeble?\n",
      "    a) reinforced\n",
      "    b) weak\n",
      "    c) damage\n",
      "    d) break\n",
      "you answered: weak \n",
      "\n",
      "4. What is a synonym for administer?\n",
      "    a) give\n",
      "    b) steal\n",
      "    c) spray\n",
      "    d) box\n",
      "you answered: give \n",
      "\n",
      "5. What is a synonym for betray?\n",
      "    a) trust\n",
      "    b) inform\n",
      "    c) table\n",
      "    d) deceive\n",
      "you answered: deceive \n",
      "\n",
      "6. What is a synonym for scour?\n",
      "    a) search\n",
      "    b) allow\n",
      "    c) gaze\n",
      "    d) gather\n",
      "you answered: search \n",
      "\n",
      "7. What is a synonym for clean?\n",
      "    a) bare\n",
      "    b) tidy\n",
      "    c) rummage\n",
      "    d) pop\n",
      "you answered: bare \n",
      "\n",
      "8. What is a synonym for abscond?\n",
      "    a) escape\n",
      "    b) rally\n",
      "    c) relinquish\n",
      "    d) flash\n",
      "you answered: relinquish \n",
      "\n",
      "9. What is a synonym for envelop?\n",
      "    a) uncover\n",
      "    b) dry\n",
      "    c) conceal\n",
      "    d) surround\n",
      "you answered: surround \n",
      "\n",
      "10. What is a synonym for curb?\n",
      "    a) keep\n",
      "    b) find\n",
      "    c) limit\n",
      "    d) grow\n",
      "you answered: limit \n",
      "\n",
      "11. What is a synonym for oblivious?\n",
      "    a) unaware\n",
      "    b) mindful\n",
      "    c) precarious\n",
      "    d) careful\n",
      "you answered: unaware \n",
      "\n",
      "12. What is a synonym for corroborate?\n",
      "    a) condemn\n",
      "    b) grieve\n",
      "    c) delight\n",
      "    d) confirm\n",
      "you answered: confirm \n",
      "\n",
      "13. What is a synonym for capricious?\n",
      "    a) stable\n",
      "    b) unaware\n",
      "    c) fickle\n",
      "    d) eager\n",
      "you answered: fickle \n",
      "\n",
      "14. What is a synonym for imminent?\n",
      "    a) impending\n",
      "    b) possible\n",
      "    c) moderate\n",
      "    d) unlikely\n",
      "you answered: impending \n",
      "\n",
      "15. What is a synonym for lucrative?\n",
      "    a) honorable\n",
      "    b) profitable\n",
      "    c) exciting\n",
      "    d) predictable\n",
      "you answered: profitable \n",
      "\n",
      "16. What is a synonym for languish?\n",
      "    a) work\n",
      "    b) avoid\n",
      "    c) attempt\n",
      "    d) suffer\n",
      "you answered: avoid \n",
      "\n",
      "17. What is a synonym for incoherent?\n",
      "    a) unclear\n",
      "    b) quiet\n",
      "    c) wise\n",
      "    d) unfeeling\n",
      "you answered: unfeeling \n",
      "\n",
      "18. What is a synonym for industrious?\n",
      "    a) energetic\n",
      "    b) prompt\n",
      "    c) diligent\n",
      "    d) headstrong\n",
      "you answered: headstrong \n",
      "\n",
      "19. What is a synonym for paucity?\n",
      "    a) envy\n",
      "    b) greed\n",
      "    c) shortage\n",
      "    d) presence\n",
      "you answered: envy \n",
      "\n",
      "20. What is a synonym for ambush?\n",
      "    a) surprise\n",
      "    b) warn\n",
      "    c) treat\n",
      "    d) announce\n",
      "you answered: surprise \n",
      "\n",
      "21. What is a synonym for propose?\n",
      "    a) suggest\n",
      "    b) doubt\n",
      "    c) strive\n",
      "    d) write\n",
      "you answered: suggest \n",
      "\n",
      "22. What is a synonym for vivid?\n",
      "    a) bright\n",
      "    b) dull\n",
      "    c) visible\n",
      "    d) loud\n",
      "you answered: bright \n",
      "\n",
      "23. What is a synonym for keen?\n",
      "    a) visible\n",
      "    b) omnipotent\n",
      "    c) metal\n",
      "    d) sharp\n",
      "you answered: sharp \n",
      "\n",
      "24. What is a synonym for bestow?\n",
      "    a) grant\n",
      "    b) cost\n",
      "    c) wipe\n",
      "    d) rise\n",
      "you answered: grant \n",
      "\n",
      "25. What is a synonym for cogitate?\n",
      "    a) mirror\n",
      "    b) contemplate\n",
      "    c) discuss\n",
      "    d) sleep\n",
      "you answered: contemplate \n",
      "\n",
      "26. What is a synonym for amble?\n",
      "    a) escape\n",
      "    b) ascend\n",
      "    c) groom\n",
      "    d) stroll\n",
      "you answered: stroll \n",
      "\n",
      "27. What is a synonym for trajectory?\n",
      "    a) goal\n",
      "    b) orbit\n",
      "    c) cut\n",
      "    d) angle\n",
      "you answered: angle \n",
      "\n",
      "28. What is a synonym for annex?\n",
      "    a) conquer\n",
      "    b) limit\n",
      "    c) jump\n",
      "    d) hide\n",
      "you answered: hide \n",
      "\n",
      "29. What is a synonym for capitulate?\n",
      "    a) defeat\n",
      "    b) concede\n",
      "    c) raise\n",
      "    d) climb\n",
      "you answered: concede \n",
      "\n",
      "30. What is a synonym for sanguine?\n",
      "    a) pessimistic\n",
      "    b) unsure\n",
      "    c) sad\n",
      "    d) positive\n",
      "you answered: pessimistic \n",
      "\n",
      "Answering part 1 using cosine similarity as the comparison metric...\n",
      "1. What is a synonym for gullible?\n",
      "    a) unrealistic\n",
      "    b) naive\n",
      "    c) complicated\n",
      "    d) wary\n",
      "you answered: naive \n",
      "\n",
      "2. What is a synonym for counter?\n",
      "    a) parry\n",
      "    b) agree\n",
      "    c) hold\n",
      "    d) run\n",
      "you answered: hold \n",
      "\n",
      "3. What is a synonym for feeble?\n",
      "    a) reinforced\n",
      "    b) weak\n",
      "    c) damage\n",
      "    d) break\n",
      "you answered: weak \n",
      "\n",
      "4. What is a synonym for administer?\n",
      "    a) give\n",
      "    b) steal\n",
      "    c) spray\n",
      "    d) box\n",
      "you answered: give \n",
      "\n",
      "5. What is a synonym for betray?\n",
      "    a) trust\n",
      "    b) inform\n",
      "    c) table\n",
      "    d) deceive\n",
      "you answered: deceive \n",
      "\n",
      "6. What is a synonym for scour?\n",
      "    a) search\n",
      "    b) allow\n",
      "    c) gaze\n",
      "    d) gather\n",
      "you answered: search \n",
      "\n",
      "7. What is a synonym for clean?\n",
      "    a) bare\n",
      "    b) tidy\n",
      "    c) rummage\n",
      "    d) pop\n",
      "you answered: bare \n",
      "\n",
      "8. What is a synonym for abscond?\n",
      "    a) escape\n",
      "    b) rally\n",
      "    c) relinquish\n",
      "    d) flash\n",
      "you answered: relinquish \n",
      "\n",
      "9. What is a synonym for envelop?\n",
      "    a) uncover\n",
      "    b) dry\n",
      "    c) conceal\n",
      "    d) surround\n",
      "you answered: surround \n",
      "\n",
      "10. What is a synonym for curb?\n",
      "    a) keep\n",
      "    b) find\n",
      "    c) limit\n",
      "    d) grow\n",
      "you answered: limit \n",
      "\n",
      "11. What is a synonym for oblivious?\n",
      "    a) unaware\n",
      "    b) mindful\n",
      "    c) precarious\n",
      "    d) careful\n",
      "you answered: unaware \n",
      "\n",
      "12. What is a synonym for corroborate?\n",
      "    a) condemn\n",
      "    b) grieve\n",
      "    c) delight\n",
      "    d) confirm\n",
      "you answered: confirm \n",
      "\n",
      "13. What is a synonym for capricious?\n",
      "    a) stable\n",
      "    b) unaware\n",
      "    c) fickle\n",
      "    d) eager\n",
      "you answered: fickle \n",
      "\n",
      "14. What is a synonym for imminent?\n",
      "    a) impending\n",
      "    b) possible\n",
      "    c) moderate\n",
      "    d) unlikely\n",
      "you answered: impending \n",
      "\n",
      "15. What is a synonym for lucrative?\n",
      "    a) honorable\n",
      "    b) profitable\n",
      "    c) exciting\n",
      "    d) predictable\n",
      "you answered: profitable \n",
      "\n",
      "16. What is a synonym for languish?\n",
      "    a) work\n",
      "    b) avoid\n",
      "    c) attempt\n",
      "    d) suffer\n",
      "you answered: suffer \n",
      "\n",
      "17. What is a synonym for incoherent?\n",
      "    a) unclear\n",
      "    b) quiet\n",
      "    c) wise\n",
      "    d) unfeeling\n",
      "you answered: unfeeling \n",
      "\n",
      "18. What is a synonym for industrious?\n",
      "    a) energetic\n",
      "    b) prompt\n",
      "    c) diligent\n",
      "    d) headstrong\n",
      "you answered: diligent \n",
      "\n",
      "19. What is a synonym for paucity?\n",
      "    a) envy\n",
      "    b) greed\n",
      "    c) shortage\n",
      "    d) presence\n",
      "you answered: shortage \n",
      "\n",
      "20. What is a synonym for ambush?\n",
      "    a) surprise\n",
      "    b) warn\n",
      "    c) treat\n",
      "    d) announce\n",
      "you answered: surprise \n",
      "\n",
      "21. What is a synonym for propose?\n",
      "    a) suggest\n",
      "    b) doubt\n",
      "    c) strive\n",
      "    d) write\n",
      "you answered: suggest \n",
      "\n",
      "22. What is a synonym for vivid?\n",
      "    a) bright\n",
      "    b) dull\n",
      "    c) visible\n",
      "    d) loud\n",
      "you answered: bright \n",
      "\n",
      "23. What is a synonym for keen?\n",
      "    a) visible\n",
      "    b) omnipotent\n",
      "    c) metal\n",
      "    d) sharp\n",
      "you answered: sharp \n",
      "\n",
      "24. What is a synonym for bestow?\n",
      "    a) grant\n",
      "    b) cost\n",
      "    c) wipe\n",
      "    d) rise\n",
      "you answered: grant \n",
      "\n",
      "25. What is a synonym for cogitate?\n",
      "    a) mirror\n",
      "    b) contemplate\n",
      "    c) discuss\n",
      "    d) sleep\n",
      "you answered: contemplate \n",
      "\n",
      "26. What is a synonym for amble?\n",
      "    a) escape\n",
      "    b) ascend\n",
      "    c) groom\n",
      "    d) stroll\n",
      "you answered: stroll \n",
      "\n",
      "27. What is a synonym for trajectory?\n",
      "    a) goal\n",
      "    b) orbit\n",
      "    c) cut\n",
      "    d) angle\n",
      "you answered: orbit \n",
      "\n",
      "28. What is a synonym for annex?\n",
      "    a) conquer\n",
      "    b) limit\n",
      "    c) jump\n",
      "    d) hide\n",
      "you answered: conquer \n",
      "\n",
      "29. What is a synonym for capitulate?\n",
      "    a) defeat\n",
      "    b) concede\n",
      "    c) raise\n",
      "    d) climb\n",
      "you answered: concede \n",
      "\n",
      "30. What is a synonym for sanguine?\n",
      "    a) pessimistic\n",
      "    b) unsure\n",
      "    c) sad\n",
      "    d) positive\n",
      "you answered: pessimistic \n",
      "\n",
      "accuracy using euclidean distance: 0.66667\n",
      "accuracy using cosine similarity : 0.83333\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666, 0.8333333333333334)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This will create a class to test the functions you implemented above. If you are curious, \n",
    "you can see the code for this in quizlet.py but it is not required. If you run this cell,\n",
    "we will load the test data for you and run it on your functions to test your implementation.\n",
    "\n",
    "You should get an accuracy of 66% with euclidean distance and 83% with cosine distance\n",
    "\"\"\"\n",
    "\n",
    "part1 = quizlet.Part1_Runner(find_synonym, part1_written)\n",
    "part1.evaluate(True)  # To only print the scores, pass in False as an argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Exploration\n",
    "In this section, you'll do an exploration question. Specifically, you'll implement the following 2 functions:\n",
    "\n",
    "* **occupation_exploration()**: given a list of occupations, find the top 5 occupations with the highest cosine similarity to the word \"man\", and the top 5 occupations with the highest cosine similarity to the word \"woman\".\n",
    "* **part2_written()**: look at your results from the previous exploration task. What do you observe, and why do you think this might be the case? Write your answer within the function by returning a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occupation_exploration(occupations, embeddings):\n",
    "    '''\n",
    "    Given a list of occupations, return the 5 occupations that are closest\n",
    "    to 'man', and the 5 closest to 'woman', using cosine similarity between\n",
    "    corresponding word embeddings as a measure of similarity.\n",
    "\n",
    "    Arguments:\n",
    "        occupations (List[str]): list of occupations\n",
    "        embeddings (Dict[str, np.array]): map of words (strings) to their embeddings (np.array)\n",
    "\n",
    "    Returns:\n",
    "        top_man_occs (List[str]): list of 5 occupations closest to 'man'\n",
    "        top_woman_occs (List[str]): list of 5 occuptions closest to 'woman'\n",
    "            note: both lists should be sorted, with the occupation with highest\n",
    "                  cosine similarity first in the list\n",
    "    '''\n",
    "    top_man_occs = []\n",
    "    top_woman_occs = []\n",
    "    #########################################################\n",
    "    ## TODO: get 5 occupations closest to 'man' & 'woman'  ##\n",
    "    #########################################################\n",
    "    f_dict = {}\n",
    "    for word in ['man', 'woman']:\n",
    "        word_embedding = embeddings[word]                 \n",
    "        a_dict = {}\n",
    "        for occupation in occupations:\n",
    "            a_dict[occupation] = \\\n",
    "                cosine_similarity(word_embedding, embeddings[occupation])\n",
    "            pass\n",
    "        n_dict = dict(sorted(a_dict.items(), \n",
    "                             key = lambda item: item[1], reverse=True)[:5])\n",
    "        f_dict[word] = n_dict.keys()        \n",
    "        pass\n",
    "    top_man_occs = f_dict['man']        \n",
    "    top_woman_occs = f_dict['woman']\n",
    "    #########################################################\n",
    "    ## End TODO                                            ##\n",
    "    #########################################################\n",
    "    return top_man_occs, top_woman_occs\n",
    "\n",
    "def part2_written():\n",
    "    '''\n",
    "    Take a look at what occupations you found are closest to 'man' and\n",
    "    closest to 'woman'. Do you notice anything curious? In 1-2 sentences,\n",
    "    describe what you find, and why you think this occurs.\n",
    "    '''\n",
    "    #########################################################\n",
    "    ## TODO: replace string with your answer               ##\n",
    "    ######################################################### \n",
    "    answer = \"\"\"\n",
    "    I have not  run it\n",
    "    \"\"\"\n",
    "    #########################################################\n",
    "    ## End TODO                                            ##\n",
    "    ######################################################### \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "exploration"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2: Exploration!\n",
      "--------------------\n",
      "occupations closest to \"man\" - you answered:\n",
      " 1. teacher\n",
      " 2. actor\n",
      " 3. worker\n",
      " 4. lawyer\n",
      " 5. warrior\n",
      "occupations closest to \"woman\" - you answered:\n",
      " 1. nurse\n",
      " 2. teacher\n",
      " 3. worker\n",
      " 4. maid\n",
      " 5. waitress\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict_keys(['teacher', 'actor', 'worker', 'lawyer', 'warrior']),\n",
       " dict_keys(['nurse', 'teacher', 'worker', 'maid', 'waitress']))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part2 = quizlet.Part2_Runner(occupation_exploration, part2_written)\n",
    "part2.evaluate() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Contextual embeddings\n",
    "\n",
    "For this section, your goal is to understand contextual embeddings, which are more powerful than static embeddings. In a static embedding, we just have one vector for each word. In a contextual embedding, such as those produced by the BERT algorithm, the vector for the word is influenced by all its neighbors.  That means the  embedding for the same word is different when it appears in different sentences!   We won't study the transformer that is the core mechansim of BERT until later in the quarter, so in this assignment you are just exploring BERT as a [black box](https://en.wikipedia.org/wiki/Black_box)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased') # About 440MB large\n",
    "\n",
    "# Feel free to ignore deprecation/unused weight warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# We want to run the model on our GPU if possible, but if not, we can use a CPU\n",
    "if torch.backends.mps.is_available(): # Available on Macs with Apple silicon or AMD GPUs\n",
    "    device = torch.device(\"mps\")\n",
    "    model.to(device)\n",
    "elif torch.cuda.is_available(): # Available on computers with NVIDIA GPUs\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.to(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Model is on device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1: Contextual embedding with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_word_embedding(sentence, target_word):\n",
    "    '''\n",
    "    We implemented this function for you. It runs a sentence through BERT, and\n",
    "    returns the embedding for that word. (shape (768,))\n",
    "    '''\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    word_id = tokenizer.encode(target_word, add_special_tokens=False)\n",
    "    \n",
    "    if len(word_id) != 1:\n",
    "        raise ValueError(f\"'{target_word}' is split into multiple tokens by BERT. Please choose a simpler (~1 syllable) word.\")\n",
    "    word_id = word_id[0]\n",
    "\n",
    "    word_position = torch.where(inputs['input_ids'][0] == word_id)[0]\n",
    "    if len(word_position) == 0:\n",
    "        raise ValueError(f\"'{target_word}' not found in the sentence.\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if device is not torch.device(\"cpu\"):\n",
    "            inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state[0, word_position[0]]\n",
    "    return embedding.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Your task is to use BERT to study word polysemy (the fact that words can have multiple senses that are different from each other in meaning, like \"bat\" to mean both the flying mammal and the baseball instrument).  Your job is to find a maximally ambiguous word. We have provided an example in code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_word = \"bank\"\n",
    "example_sentence1 = f\"I went to the {example_word} to deposit my money.\"\n",
    "example_sentence2 = f\"I went down by the river {example_word} to see the ducks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This word is 54.65% similar in the two sentences.\n"
     ]
    }
   ],
   "source": [
    "def get_polyseme_similarity(word, sentence1, sentence2, return_score=False):\n",
    "    embedding1 = get_bert_word_embedding(sentence1, word)\n",
    "    embedding2 = get_bert_word_embedding(sentence2, word)\n",
    "    similarity = cosine_similarity(embedding1, embedding2)\n",
    "    if return_score:\n",
    "        return similarity\n",
    "    else:\n",
    "        print(f\"This word is {similarity*100:.2f}% similar in the two sentences.\")\n",
    "\n",
    "get_polyseme_similarity(example_word, example_sentence1, example_sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now it's your turn! Try to find a ~1 syllable [polyseme](https://prepedu.com/en/blog/polysemy-in-english) that can be used in very different contexts. You will get full points for getting it under 54% similarity. We'll have a leaderboard on gradescope for lowest similarity score achieved (In our testing, we achieved 38%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part3():\n",
    "    '''\n",
    "    Returns\n",
    "        word (str): the word used in both sentences\n",
    "        sentence1 (str): the first sentence\n",
    "        sentence2 (str): the second sentence\n",
    "\n",
    "    HINT: This word should be a polyseme, meaning it has\n",
    "    multiple meanings, and each sentence should use a different definiton.\n",
    "    '''\n",
    "    #########################################################\n",
    "    ## TODO: replace strings with your answers             ##\n",
    "    ######################################################### \n",
    "    word = \"record\"\n",
    "    sentence1 = f\"I have a {word}\"\n",
    "    sentence2 = f\"I {word} a video\"\n",
    "    #########################################################\n",
    "    ## End TODO                                            ##\n",
    "    ######################################################### \n",
    "    return word, sentence1, sentence2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 3: Contextual embeddings with BERT\n",
      "---------------------------------------\n",
      "Polyseme disambiguation: \n",
      "Word: record\n",
      "Sentence 1: I have a record\n",
      "Sentence 2: I record a video\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This word is 40.32% similar in the two sentences.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part3 = quizlet.Part3_Runner(part3, get_bert_word_embedding, cosine_similarity)\n",
    "part3.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Sentence Similarity with BERT\n",
    "\n",
    "For this section, your goal is to answer questions of the form:\n",
    "\n",
    "- How semantically similar are the following two sentences?:\n",
    "\n",
    "    - he later learned that the incident was caused by the concorde's sonic boom\n",
    "\n",
    "    - he later found out the alarming incident had been caused by concorde's powerful sonic boom\n",
    "\n",
    "## Part 4.1: Sentence-level embeddings with BERT\n",
    "\n",
    "In this section, we will be leveraging the BERT model for a sentence classification task. In the real world, many applications of semantic understanding are done with fine-tuned transformer models, and we will be using a simple BERT model that was trained by Google on [BookCorpus](https://en.wikipedia.org/wiki/BookCorpus). To efficiently get the embeddings for multiple sentences, we will implement `get_bert_sentence_embeddings()`\n",
    "\n",
    "Our `get_bert_sentence_embeddings()` function takes in two parameters besides our inputs. The `batch_size` parameter exists to limit memory usage, which is necessary if you wanted to use this function to compute embeddings on an even larger dataset. (Feel free to try it yourself)! The boolean `use_CLS` explains which of the two following methods we will use for classifying a document:\n",
    "* **Use the final [CLS] token embedding**: The first token represents the combined context of the full sentence, so we will simply compare this one token across sentences\n",
    "* **Mean pooling over all sentence tokens**: We will average the token embeddings in the last hidden layer of our BERT outputs. Calculating this is a bit complex, so we've done a lot of the steps for you already. Each step is explained with comments, but for each sentence, we are summing the outputs for each token, but only where the token is not a padding token.\n",
    "\n",
    "Some (hopefully) helpful hints!\n",
    "- For extracting the [CLS] token embeddings:\n",
    "  - We want an output of shape (n_sentences, 768), and the shape of `outputs.last_hidden_state` is  (n_sentences, sequence_length, 768). The reason it is length 768 is because at the last hidden layer of the output in BERT, each token is represented by a vector of this length. If you do not know how multi-dimensional slicing works in NumPy/PyTorch, this guide may be helpful: [Python Slice Indexing](https://www.geeksforgeeks.org/python-slicing-multi-dimensional-arrays/) \n",
    "- For getting the mean of all tokens in the output\n",
    "  - We have implemented the hard part of mean pooling already, and we documented it in the comments. Since we are passing in sentences of varying lenghts, all sentences are padded with [PAD] tokens that we wish to ignore. We use a mask to zero-out the embeddings for the [PAD] tokens, and we also ignore them in our total count by summing the mask. \n",
    "  - What is left to do is take the mean of these filtered embeddings. To do so, we sum along the sequence axis, then divide by the provided sum_mask variable.\n",
    "- Want more visualization for the outputs of BERT? This [Illustrated Guide to BERT](http://jalammar.github.io/illustrated-bert/) is a recommended reading from [CS224n](https://web.stanford.edu/class/cs224n/index.html#schedule). The section in the linked YouTube video at the timestamp 2:52 is a good example of passing one sentence through BERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_sentence_embeddings(sentences, use_CLS=True, batch_size=8):\n",
    "    '''\n",
    "    Generate embeddings for sentences using BERT's CLS token.\n",
    "    \n",
    "    Arguments:\n",
    "        sentences (List[str]): Input sentences.\n",
    "        use_CLS (bool): Whether to use the CLS token as the sentence embedding.\n",
    "                        If it is false, we use mean pooling over sentence tokens.\n",
    "        batch_size (int): Batch size for processing the sentences.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Sentence embeddings of shape (n_sentences, 768).\n",
    "    '''\n",
    "    all_embeddings = []\n",
    "    # We process the sentences in batches to avoid running out of memory. \n",
    "    # Feel free to experiment with the batch size, 8 or 16 are likely best.\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "        if device is not torch.device(\"cpu\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to your GPU\n",
    "        with torch.no_grad(): # Runs the model without calculating gradients\n",
    "            outputs = model(**inputs) # shape: (batch_size, max_sentence_length, 768)\n",
    "        embeddings = None\n",
    "        if use_CLS:\n",
    "            #########################################################\n",
    "            ### TODO: Extract each CLS token embedding from the     #\n",
    "            #         output of each sentence                       #\n",
    "            #########################################################\n",
    "            ### BEGIN CODE HERE (~1 line) ###\n",
    "            embeddings = outputs[:, 0, :]\n",
    "            ### END CODE HERE ###\n",
    "        else:\n",
    "            # We first create a mask to distinguish real tokens from padding tokens\n",
    "            attention_mask = inputs['attention_mask']\n",
    "            # We then expand the mask to the same shape as the embeddings\n",
    "            mask_expanded = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "            # Sum the 1s in the mask to get the number of non-padding tokens for each sentence\n",
    "            sum_mask = mask_expanded.sum(dim=1)\n",
    "            # Clamp the sum to 1e-9 to avoid division by zero\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "            # Apply the mask to the embeddings, so that the padding tokens are ignored\n",
    "            masked_embeddings = outputs.last_hidden_state * mask_expanded\n",
    "            #########################################################\n",
    "            ### TODO: Extract the mean of all token embeddings      #\n",
    "            #         by summing the embeddings along the sequence  #\n",
    "            #         dimension and dividing by the sum_mask        #\n",
    "            #########################################################\n",
    "            ### BEGIN CODE HERE (~1-2 lines) ###\n",
    "\n",
    "            embeddings = None\n",
    "            ### END CODE HERE ###\n",
    "        all_embeddings.append(embeddings)\n",
    "    \n",
    "    embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    return embeddings.cpu().numpy() # Numpy does not support GPU tensors, so we move it to the CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Sanity check\n",
    "Test out your implementations!\n",
    "You should find that sentences 2 and 3 are quite similar to each other (>97% on CLS similarity, >90% on mean pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\12-19-11-1-2023\\Downloads\\pa5-embeddings-main\\pa5.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sentences \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mTo be or not to be, that is the question.\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m              \u001b[39m\"\u001b[39m\u001b[39mThe feline sat on the rug.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mThe cat sat on the mat.\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m embeddings \u001b[39m=\u001b[39m get_bert_sentence_embeddings(sentences, use_CLS\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m cos_sim_12 \u001b[39m=\u001b[39m cosine_similarity(embeddings[\u001b[39m0\u001b[39m], embeddings[\u001b[39m1\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m cos_sim_13 \u001b[39m=\u001b[39m cosine_similarity(embeddings[\u001b[39m0\u001b[39m], embeddings[\u001b[39m2\u001b[39m])\n",
      "\u001b[1;32mc:\\Users\\12-19-11-1-2023\\Downloads\\pa5-embeddings-main\\pa5.ipynb Cell 28\u001b[0m in \u001b[0;36mget_bert_sentence_embeddings\u001b[1;34m(sentences, use_CLS, batch_size)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m use_CLS:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m#########################################################\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m### TODO: Extract each CLS token embedding from the     #\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m#         output of each sentence                       #\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m#########################################################\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m### BEGIN CODE HERE (~1 line) ###\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     embeddings \u001b[39m=\u001b[39m outputs[:, \u001b[39m0\u001b[39;49m, :]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39m### END CODE HERE ###\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39m# We first create a mask to distinguish real tokens from padding tokens\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12-19-11-1-2023/Downloads/pa5-embeddings-main/pa5.ipynb#X40sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     attention_mask \u001b[39m=\u001b[39m inputs[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32md:\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\generic.py:435\u001b[0m, in \u001b[0;36mModelOutput.__getitem__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_dict[k]\n\u001b[0;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_tuple()[k]\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "sentences = [\"To be or not to be, that is the question.\", \n",
    "             \"The feline sat on the rug.\", \"The cat sat on the mat.\"]\n",
    "\n",
    "embeddings = get_bert_sentence_embeddings(sentences, use_CLS=True)\n",
    "\n",
    "cos_sim_12 = cosine_similarity(embeddings[0], embeddings[1])\n",
    "cos_sim_13 = cosine_similarity(embeddings[0], embeddings[2])\n",
    "cos_sim_23 = cosine_similarity(embeddings[1], embeddings[2])\n",
    "print(f\"Similarity between s1 and s2: {cos_sim_12*100:.2f}%\")\n",
    "print(f\"Similarity between s1 and s3: {cos_sim_13*100:.2f}%\")\n",
    "print(f\"Similarity between s2 and s3: {cos_sim_23*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3: Implementing a simple search engine with BERT\n",
    "\n",
    "Now that we can compare the similarity of documents, we can implement a very rudimetary search engine that compares the embedding of a search query to the embedding of documents in our corpus. While modern web retrieval is much more complicated and has a million more optimizations, fine-tuned versions of BERT empower many things you may use in your daily life, such as search engines.\n",
    "\n",
    "### The task:\n",
    "We will be using StanfordAI's web_questions dataset (from [this paper](https://cs.stanford.edu/~pliang/papers/freebase-emnlp2013.pdf)). It is comprised of many question/answer pairs that you can look at further in depth on [HuggingFace](https://huggingface.co/datasets/Stanford/web_questions).\n",
    "\n",
    "Our task is simple. Given a query, we want to find the highest cosine similarities between other queries that exist, and then return the resulting answer to that query. To simulate a Google search page, we will return the top_k results, where k is some number we specify.\n",
    "\n",
    "This will be done in about 5 steps\n",
    "- In the \\_\\_init\\_\\_() function, you will first get the matrix embedding for all of the questions in our dataset. Currently, the dataset is a dictionary of { question : List[answer] } key/value pairs. You need to pass the questions as a list of sentences into get_bert_sentence_embeddings(). Ignore the answers for now.\n",
    "- In the web_search() function, we want to first get the embedding for our query. You may need to pass it into your model function as a list.\n",
    "- We then want to compute the cosine similarity between our query embedding and our question embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlike your previous cosine sim function, this one works with matrices\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine_similarity_2d\n",
    "\n",
    "class BertWebSearch():\n",
    "    def __init__(self):\n",
    "        # This is a dict with ~3k { question : answer } key pairs.\n",
    "        self.question_answer_dict = quizlet.load_stanford_web_questions()\n",
    "\n",
    "        #######################################################\n",
    "        ### TODO: Get the embeddings for all the questions  ###\n",
    "        #         in the question_answer_dict.                #\n",
    "        #######################################################\n",
    "        ### BEGIN CODE HERE (~3 lines) ###\n",
    "        self.questions = None\n",
    "        self.question_embeddings_CLS = None\n",
    "        self.question_embeddings_pooling = None\n",
    "        #######################################################\n",
    "        ### END CODE HERE                                   ###\n",
    "        #######################################################\n",
    "        n_questions = len(self.questions)\n",
    "        assert self.question_embeddings_CLS.shape == (n_questions, 768)\n",
    "\n",
    "    def search_web(self, query, k=5, use_CLS=True):\n",
    "        \"\"\"\n",
    "        Returns the top K question/answer pairs most similar to the input query, based on BERT embeddings.\n",
    "\n",
    "        Arguments:\n",
    "            query (str): The search query.\n",
    "\n",
    "        Returns:\n",
    "            top_questions (List[str]): The top K questions most similar to the query.\n",
    "            top_answers (List[str]): The corresponding answers to the top K questions.\n",
    "        \"\"\"\n",
    "        #########################################################\n",
    "        ### TODO: Get the embedding for the query and find the  #\n",
    "        #         cosine similarity between the query and all   #\n",
    "        #         questions. Return the top k questions and     #\n",
    "        #         their answers                                 #\n",
    "        #########################################################\n",
    "        ### BEGIN CODE HERE (~6-8 lines) ###\n",
    "        ### HINT: use the cosine_similarity_2d function between your query and the question embeddings you stored above.\n",
    "        \n",
    "        top_questions, top_answers = [], [] # you'll likely want to delete this line\n",
    "        ### END CODE HERE ###\n",
    "        return top_questions, top_answers\n",
    "    \n",
    "    def test_web_search(self, query, k, use_CLS=True):\n",
    "        top_questions, top_answers = self.search_web(query, k, use_CLS)\n",
    "        print(f\"Top {k} questions similar to '{query}':\")\n",
    "        for question, answer in zip(top_questions, top_answers):\n",
    "            print(f\"- Did you mean.... {question}\")\n",
    "            print(f\"    Answer(s):  {answer}\")\n",
    "\n",
    "search_engine = BertWebSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What do people speak in Spain\"\n",
    "search_engine.test_web_search(query, 5, use_CLS=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test our web search function on three queries. You get full points if it runs, but you should see some semantic similarity between your query and the returned questions. Try testing it on some queries of your own! And experiment with the `use_CLS` boolean, see if you notice any qualitative differences in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part4 = quizlet.Part4_Runner(search_engine.search_web, get_bert_sentence_embeddings).evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congrats on finishing!\n",
    "\n",
    "As a parting thought, we hope that these past several assignments have got you thinking about how large scale algorithms on text function and how we can improve them at scale. We only implemented small parts at a time, but hopefully these foundations are helpful in thinking about how language modeling algorithms shape how information is stored and retrieved online.\n",
    "\n",
    "### If you collaborated with a partner, describe below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaboration():\n",
    "    '''\n",
    "    Returns:\n",
    "        answer (str): what you and your partner did each / together\n",
    "    '''\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission and Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're ready to submit, you can run the cell below to prepare and zip up your solution:\n",
    "\n",
    "If you're running on Google Colab, see the README for instructions on how to submit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -f \"./pa5.ipynb\" ]]\n",
    "then\n",
    "    echo \"WARNING: Did not find notebook in Jupyter working directory. This probably means you're running on Google Colab. You'll need to go to File->Download .ipynb to download your notebok and other files, then zip them locally. See the README for more information.\"\n",
    "else\n",
    "    echo \"Found notebook file, creating submission zip...\"\n",
    "    zip -r submission.zip pa5.ipynb deps/\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Some reminders for submission:__\n",
    " * Make sure you didn't accidentally change the name of your notebook file, (it should be `pa5.ipynb`) as that is required for the autograder to work.\n",
    "* Go to Gradescope (gradescope.com), find the PA5 Quizlet assignment and upload your zip file (`submission.zip`) as your solution.\n",
    "* Wait for the autograder to run and check that your submission was graded successfully! If the autograder fails, or you get an unexpected score it may be a sign that your zip file was incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to delete BERT from your computer:\n",
    "\n",
    "On MacOS, navigate in Finder to your huggingface cache (likely `cd ~/.cache/huggingface/`)\n",
    "\n",
    "use `ls` to see what files are there. It may be inside another folder, e.g., `hub`\n",
    "\n",
    "find and delete the model `rm -rf models--bert-base-uncased`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can't find your cache? This prints the path to the cache directory where the HuggingFace models are stored\n",
    "print(file_utils.default_cache_path)\n",
    "# Within this directory, it may be in a folder named 'transformers' or 'models--bert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This directory is hidden by default on MacOS. With Finder open, you can use the 'Go' menu (or the Shift+Command+G macro) and type in the path for the folder you want to reach. You should also know that to toggle the Show Hidden Files option in Finder, the keybind/macro is: (Shift+Command+Period). You can delete it with the graphical interface of Finder, or in a terminal window / jupyter code cell using the aforementioned command `rm -rf <file/folder>`. Fun fact! on MacOS, you can open a new terminal session at a specific directory by dragging that folder from Finder onto the terminal icon"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "cs124at",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
